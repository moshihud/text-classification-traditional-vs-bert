# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ck6r9iVxJhJ-aBX5Isi2vM6xN5sjcro0

### Environment Setup

The `contractions` package is download required, which is not available by default in Google Colab. Other libraries (e.g., NLTK, scikit-learn, Transformers) are preinstalled
"""

!pip install contractions

"""### Library Imports

All necessary libraries for text preprocessing, data handling, model training, and evaluation are imported. This ensures single runnable file.

"""

import os
import contractions
import html
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pickle
import re

from bs4 import BeautifulSoup
from google.colab import drive

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, accuracy_score
from sklearn.model_selection import ParameterGrid

from transformers import BertTokenizer, BertModel
import torch

nltk.download("punkt_tab")
nltk.download("stopwords")
nltk.download("wordnet")

"""### Reproducibility

Setting up global NumPy seed acording to my `student_id` (2412240) to ensure all randomness is reproducible as required.

"""

student_id = 2412240

# Numpy seed
np.random.seed(student_id)

# Mount Google Drive
drive.mount("/content/drive", force_remount=True)

# Add your code to initialize GDrive and data and models paths

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = "./CE807-25-SU/Assignment/"
GOOGLE_DRIVE_PATH = os.path.join("drive", "MyDrive", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print("List files: ", os.listdir(GOOGLE_DRIVE_PATH))

DATA_PATH = os.path.join(GOOGLE_DRIVE_PATH, "data", "32")
train_file = os.path.join(DATA_PATH, "train.csv")
print("Train file: ", train_file)

val_file = os.path.join(DATA_PATH, "valid.csv")
print("Validation file: ", val_file)

test_file = os.path.join(DATA_PATH, "test.csv")
print("Test file: ", test_file)

train_df = pd.read_csv(train_file)
valid_df = pd.read_csv(val_file)
test_df = pd.read_csv(test_file)

MODEL_PATH_1 = os.path.join(GOOGLE_DRIVE_PATH, "model", str(student_id), "model_1")
os.makedirs(MODEL_PATH_1, exist_ok=True)

MODEL_PATH_2 = os.path.join(GOOGLE_DRIVE_PATH, "model", str(student_id), "model_2")
os.makedirs(MODEL_PATH_2, exist_ok=True)

train_df.head()

train_df.info()

train_df.describe()

"""### Class Imbalance Observation

A histogram of the `rating` column reveals a strong class imbalance, with rating 5 dominating. This indicate that we should use `class_weight='balanced'` in logistic regression to handle data.

"""

train_df["rating"].value_counts().sort_index().plot(kind="bar")
plt.title("Class Distribution of Ratings")
plt.xlabel("Rating")
plt.ylabel("Count")
plt.show()

train_df["word_count"] = train_df["text"].apply(lambda x: len(x.split()))
train_df["word_count"].describe()

"""### Stopword Distribution Analysis

We calculate the number of stopwords in each review to understand low-information content. This led the decision to remove stopwords during traditional preprocessing to enhance model focus.

"""

stop_words = set(stopwords.words("english"))

def count_stopwords(text):
    tokens = word_tokenize(text.lower())
    return sum(1 for word in tokens if word in stop_words)

train_df["stopword_count"] = train_df["text"].apply(count_stopwords)
train_df["stopword_count"].describe()

"""### Custom Preprocessing Justification

This pipeline is designed for traditional ML models (TF-IDF + LR). It removes HTML, expands contractions, lowercases, strips punctuation and digits, and performs tokenization, stopword removal, and lemmatization. We use lemmatization instead of stemming to preserve word meaning, which improves model interpretability for TF-IDF.

"""

# Initialize stopword list and lemmatizer
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    # Decode HTML entities
    text = html.unescape(text)

    # Remove HTML tags (e.g. <br>)
    text = BeautifulSoup(text, "html.parser").get_text(separator=" ")

    # Expand contractions (e.g. don't ==> do not)
    text = contractions.fix(text)

    # Lowercase the text
    text = text.lower()

    # Remove punctuation and numbers
    text = re.sub(r"[^\w\s]", "", text)
    text = re.sub(r"\d+", "", text)

    tokens = word_tokenize(text)

    # Remove stopwords
    tokens = [word for word in tokens if word not in stop_words]

    # Apply lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Rejoin and normalize whitespace
    return " ".join(tokens).strip()

def read_data(path):
    df = pd.read_csv(path)
    df["text"] = df["text"].astype(str)
    print(path, "has", len(df), "data points")
    return df

"""### Training Justification for TF-IDF + Logistic Regression

This function implements a traditional text classification pipeline using TF-IDF vectorization and Logistic Regression. We apply grid search over hyperparameters including `ngram_range`, `min_df`, `C`, and `solver` to systematically explore model configurations because without it the model accuracy was very low. And the TF-IDF representation is chosen for its sparse and interpretable features that align well with linear classifiers. To address the observed class imbalance, `class_weight='balanced'` is used during training. Validation performance is evaluated using weighted F1-score to ensure fairness across all rating classes, and the best model and vectorizer are saved for later use.

"""

def train_1(train_file, val_file, model_dir):
    print("Training Method 1 (TF-IDF + Logistic Regression) with Grid Search...")
    train_df = read_data(train_file)
    valid_df = read_data(val_file)

    train_df["clean_text"] = train_df["text"].apply(preprocess)
    valid_df["clean_text"] = valid_df["text"].apply(preprocess)

    param_grid = {
        "ngram_range": [(1, 1), (1, 2)],
        "min_df": [1, 3],
        "C": [0.01, 0.1, 1, 10],
        "solver": ["lbfgs", "liblinear"],
    }

    best_f1 = 0
    best_params = None

    print("Grid searching...")
    for params in ParameterGrid(param_grid):
        vectorizer = TfidfVectorizer(
            ngram_range=params["ngram_range"],
            stop_words="english",
            min_df=params["min_df"],
        )
        X_train = vectorizer.fit_transform(train_df["clean_text"])
        X_valid = vectorizer.transform(valid_df["clean_text"])

        clf = LogisticRegression(
            C=params["C"],
            solver=params["solver"],
            class_weight="balanced",
            random_state=student_id,
            max_iter=1000,
        )
        clf.fit(X_train, train_df["rating"])
        preds = clf.predict(X_valid)
        f1 = f1_score(valid_df["rating"], preds, average="weighted")

        print(f"Params: {params}, F1: {f1:.4f}")
        if f1 > best_f1:
            best_f1 = f1
            best_params = params
            best_model = clf
            best_vectorizer = vectorizer

    print("\n Grid Search Complete.")
    print("\nBest Params:", best_params)
    print("Best F1 Score:", best_f1)

    # Final evaluation on validation set using best model
    X_val_best = best_vectorizer.transform(valid_df["clean_text"])
    y_val = valid_df["rating"]
    val_preds = best_model.predict(X_val_best)

    print("\nClassification Report for Best Model (TF-IDF + LR):")
    print(classification_report(y_val, val_preds, labels=[1, 2, 3, 4, 5]))

    val_acc = accuracy_score(y_val, val_preds)
    print(f"Validation Accuracy: {val_acc:.4f}")

    model_file = os.path.join(model_dir, "model.sav")
    pickle.dump(best_model, open(model_file, "wb"))
    print("Saved model to", model_file)

    vocab_file = os.path.join(model_dir, "vocab.sav")
    pickle.dump(best_vectorizer, open(vocab_file, "wb"))
    print("Saved vocab to", vocab_file)

def test_1(test_file, model_dir):
    test_df = read_data(test_file)
    test_df["clean_text"] = test_df["text"].apply(preprocess)

    model_file = os.path.join(model_dir, "model.sav")
    classifier = pickle.load(open(model_file, "rb"))
    print("Model loaded from", model_file)

    vocab_file = os.path.join(model_dir, "vocab.sav")
    tfidf_vectorizer = pickle.load(open(vocab_file, "rb"))
    print("Vocab loaded from", vocab_file)

    X_test = tfidf_vectorizer.transform(test_df["clean_text"])
    predictions = classifier.predict(X_test)

    test_df["out_label_model_1"] = predictions
    test_df.to_csv(test_file, index=False)
    print("Saved output to", test_file)

train_1(train_file, val_file, MODEL_PATH_1)
test_1(test_file, MODEL_PATH_1)

def get_bert_cls_embeddings(texts, tokenizer, bert_model, max_len=128, batch_size=32, device="cuda"):
    embeddings = []
    bert_model.eval()
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i : i + batch_size]
        encodings = tokenizer(
            batch_texts,
            truncation=True,
            padding="max_length",
            max_length=max_len,
            return_tensors="pt",
        )
        input_ids = encodings["input_ids"].to(device)
        attention_mask = encodings["attention_mask"].to(device)
        with torch.no_grad():
            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)
            batch_embeds = outputs.pooler_output
        embeddings.append(batch_embeds.cpu().numpy())
    return np.vstack(embeddings)

"""### Training Justification for BERT + Logistic Regression

This function implements the second model using BERT-based contextual embeddings combined with Logistic Regression. For BERT we didn't do text preprocessing because BERT already know how to work without pre processed text. The pre-trained BERT model (`bert-base-uncased`) is used to extract `[CLS]` token embeddings, which capture the semantic meaning of entire reviews. We perform grid search over `C`, `solver`, and `class_weight` to select the best logistic regression configuration based on weighted F1-score on the validation set. Before this Grid search it was not performing well. It give lower score than traditioanl tokenization. So we included hyperparameter tuning. So, This approach meets the best for using deep contextualized tokenization and representation, while also handling class imbalance and ensuring model performance is optimized through validation.

"""

def train_2(train_file, val_file, model_dir):
    print("Training Method 2 (BERT + Logistic Regression) with Grid Search...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("ðŸ’» Using device:", device)

    # Load BERT model/tokenizer
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    bert_model = BertModel.from_pretrained("bert-base-uncased").to(device)

    # Read data
    train_df = read_data(train_file)
    val_df = read_data(val_file)
    train_texts = train_df["text"].astype(str).tolist()
    val_texts = val_df["text"].astype(str).tolist()
    y_train = train_df["rating"].values
    y_val = val_df["rating"].values

    # Extract BERT [CLS] features
    print("Extracting BERT [CLS] features for training set...")
    X_train = get_bert_cls_embeddings(train_texts, tokenizer, bert_model, device=device)
    print("Extracting BERT [CLS] features for validation set...")
    X_val = get_bert_cls_embeddings(val_texts, tokenizer, bert_model, device=device)

    # Hyperparameter grid
    param_grid = {
        "C": [0.01, 0.1, 1, 10],
        "solver": ["lbfgs", "liblinear", "saga"],
        "class_weight": ["balanced"],
    }

    best_f1 = 0
    best_params = None
    best_model = None

    print("\nStarting Grid Search...")
    for params in ParameterGrid(param_grid):
        clf = LogisticRegression(
            C=params["C"],
            solver=params["solver"],
            class_weight=params["class_weight"],
            random_state=student_id,
            max_iter=5000,
        )
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_val)
        f1 = f1_score(y_val, y_pred, average="weighted")
        if f1 > best_f1:
            best_f1 = f1
            best_params = params
            best_model = clf

    # Print only the best model's metrics which important for report
    print("\n Grid Search Complete.")
    print("Best Params:", best_params)
    print("Best Weighted F1:", best_f1)
    y_pred_best = best_model.predict(X_val)
    print("\nClassification Report for Best Model:\n", classification_report(y_val, y_pred_best))
    print("Best Validation Accuracy:", accuracy_score(y_val, y_pred_best))

    # Save the best model and BERT/tokenizer
    os.makedirs(model_dir, exist_ok=True)
    model_file = os.path.join(model_dir, "model.sav")
    pickle.dump(best_model, open(model_file, "wb"))
    print("Saved model to", model_dir)

def test_2(test_file, model_dir):
    print("Testing Method 2 (BERT + Logistic Regression)...")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Load BERT tokenizer and model from pretrained checkpoint
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    bert_model = BertModel.from_pretrained("bert-base-uncased").to(device)

    # Load trained classifier
    model_file = os.path.join(model_dir, "model.sav")
    clf = pickle.load(open(model_file, "rb"))
    print("Model loaded from", model_file)

    # Read and embed test data
    test_df = read_data(test_file)
    test_texts = test_df["text"].astype(str).tolist()
    print("Extracting BERT [CLS] features for test set...")
    X_test = get_bert_cls_embeddings(test_texts, tokenizer, bert_model, device=device)

    # Predict and write output
    y_pred = clf.predict(X_test)
    test_df["out_label_model_2"] = y_pred
    test_df.to_csv(test_file, index=False)
    print(f"Saved predictions to {test_file}")

train_2(train_file, val_file, MODEL_PATH_2)
test_2(test_file, MODEL_PATH_2)